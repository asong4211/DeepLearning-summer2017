{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "\n",
    "- Resources:\n",
    "    1. [An Intuitive Explannation of Convolutional Neural Networks](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)\n",
    "    2. [A Quick Introduction to Neural Network](https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/)\n",
    "    3. [Convolutional Neural Networks (LaNet)](http://deeplearning.net/tutorial/lenet.html)\n",
    "    4. [UFLDL Convolutional Neural Network Tutorial](http://ufldl.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/)\n",
    "\n",
    "\n",
    "### Motivation\n",
    "   - From Hubel and Wiesel's early work on the cat's visual cortex, we know that the visual cortex contains a complex arrangement of cells. \n",
    "   - these cells are sensitive to small sub-regions of the visual field, called a *receptive field.*\n",
    "   -  the sub-regions are tiled to cover the entire visual field. These cell acts as local filters over the input space and are well-suited to eploit the strong spatially local correlation present in natural images.\n",
    "   - Additionally, two basic cell types have been ideified: \n",
    "       - simple cells respond maximally to speciic edge-like patterns within their receptive field. \n",
    "       - complex cells have larger receptive fields and are locally invariant to the exact position of the pattern. \n",
    "    \n",
    "    \n",
    "## What is \\*Convolution\\*? \n",
    "- In Math, Convolution is essentially the blending of two functions into a third function.\n",
    "- In the context of image processing, convolution is kind of like transforming image pixels in a structured way, taking nearby pixels into account. \n",
    "- In terms of coding, think of image as a 2D array of pixels with 3 channels (RGB)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- <img src = \"convNN-overview.png\" />\n",
    "- there are 4 main operations in the ConvNet:\n",
    "    1. Convolution\n",
    "    2. Non-Linearity (ReLu)\n",
    "    3. Pooling or Sub Sampling\n",
    "    4. Classification (Fully Connected Layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Every Image can be represented using a matrix of pixel values**\n",
    "1. **channel**: a conventional term used to refer to a certain component of an image. \n",
    "    - an image usually have 3 channels: red, green and blue\n",
    "    - imagine those as three 2d-matrices stacked over each other, one for each color, each having pixel value in the range 0-255\n",
    "    - a **grayscale** image, just has one channel.\n",
    "\n",
    "\n",
    "** The Convolution Step **\n",
    "- primary purpose of Convolution in case of a ConvNet is to ** extract features from the input image**. Convolution preserves the spatial relationship between pixels by learning image features using small squares of input data. \n",
    "- In CNN terminology, the 3-by-3 matrix is called a **filter**, or kernel, or feature detector, and the matrix is formed by sliding the filter over the image and computing the dot product is called the 'Convolved Feature' or 'Activation Map' or the **'feature map'**.\n",
    "- In general, a CNN learns the values of these filters on its own during the training process. The more number of filters we have, the more image features get extracted and the better our network becomes at recognizing patterns in unseen images. \n",
    "<img src = \"ConvolutionStep.png\" />\n",
    "- the size of the Feature Map is controlled by three parameters:\n",
    "    1. **Depth**: depth correspond to the number of filters we use for the convolution operation.\n",
    "    2. **Stide**: the number of pixels by which we slide our input matrix. When the stride is 1, then we move the filters one pixel at a time. When the stride is 2, then we jumped 2 pixels at a time.\n",
    "    - having a large slide will produce smaller feature maps\n",
    "    3. **Zero-padding**\n",
    "        - sometimes it is convenient to pad the input matrix with zeros around the border, so that we can apply the filter to bordering elements of our input image matrix.\n",
    "        - a nice features of zero padding is that it allows us to control the size of the feature maps. Adding zero padding is sometimes refer to as **wide convolution**, and not using zero-padding would be a narrow convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** ReLu Non-Linearlity **\n",
    "<img src = 'ReLu.png' />\n",
    "\n",
    " - ReLu is an element wise operation ,and replaces all negative pixel values in the feature map by zero. The purpose of ReLu is to introduce non-linearlity in ConvNet, since most of the real-world data would be non-linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The Pooling Step **\n",
    "- Spatial Pooling (aka subsampling or downsampling) reduces the dimensionality of each feature map but retains the most important information.\n",
    "- In case of Max Pooling, we define a spatial neighborhood ( say 2-by-2 window) and take the largest element from the rectified feature map within that window . \n",
    "- this function of pooling is to progressively reduce the spatial size of the input representation. Pooling\n",
    "    - make the input representations  (feature dimension) smaller and more manageable\n",
    "    - reduce the number of parameters in the network, hence control overfitting\n",
    "    - make the network invariant to small transformations, distortions and translations in the input image. \n",
    "    - helps us arrive at an almost scale invariant representation of our image. \n",
    "<img src = \"maxPool.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Fully Connected Layer **\n",
    "- the fully connected layer is a traditional Multi-layer perceptron that uses a **softmax activation function ** (SVM can also be used) in the output layer. \n",
    "- \"fully connected\" implies that every neuron in the previous layer is connected to every neuron on the next layer.\n",
    "- the output from the convolutional and pooling layers represent high-level features of the input image. \n",
    "- the purpose is to use these features for classifying the input image into various classes based on the training dataset. \n",
    "- apart from classification, adding a fully-connected layer is a cheap way of learning non-linear combinations of these features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Training process of the Convolutional Neural Network:\n",
    "1. initialize all fiters and parameters/weights with random values\n",
    "2. the network takes a training image as input, goes through the forward prop step, (this includes convolution, ReLu,  pooling operation, and the fully connected layer), and finds the output probabilities for each class.\n",
    "    - let's say the output probabilities for the boat image above are [0.2,0.4,0.1,0.3]\n",
    "    - since weights are randomly assigned, the output makes sense to look random.\n",
    "3. Calculate the total error at the output layer. (summation over all classes)\n",
    "    - Total Error = $ \\sum \\frac{1}{2} (totalProbability - outputProbability) ^ 2 $\n",
    "4. Use Backprop to calculate the *gradients of the error *with respect to all weights in the network and use gradient descent to *update all filter values / weights* and parameter value to minimize the output error. \n",
    "    - the weights are adjusted in proportion to their contribution to the total error.\n",
    "    - when the same image is input again, output probabilities might now be [0.1, 0.1, 0.7, 0.1], which is close to the target vector of [0,0,1,0]\n",
    "    - this means the network has learnt to classify this particular image correctly by adjusting its weights and filters such that the output error is reduced.\n",
    "    - hyperparameters like number of fiters, fiter size, architechture of the network, etc have all been fixed before Step 1 and do not change during the training process. Only the values of the filter matrix and connection weights get updated. \n",
    "\n",
    "5. Repeat steps 2 - 4 with all images in the training set. \n",
    "    - the above steps train the Conv Net , this essentially means that all the weights and parameters of the ConvNet have now been optimized to correctly classify images from the training set. \n",
    "    - the steps above use only 2 sets of alternating convolution and pooling layers. Note that there can be repeated any number of times in a single CNN. \n",
    "    \n",
    "<img src = 'intuition.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "1. ### Review Regular NN:\n",
    "    - Traditional Neural Networks receive an input (as a single vector), and transform it through a series of hidden layers.\n",
    "    - each hidden layer is made up of a set of neurons, where each neuron is fully connected to all the neurons in the previous layer. \n",
    "    - the last fully-connected layer is called the output layer, and in classification setting it represents the class scores.\n",
    "    - the problem is: Regular Neural Nets don't scale well to full images. \n",
    "        - For example, say an image has size: 200x200x3, this would lead to neurons that have 200x200x3=120,000 weights.\n",
    "        - We would want to have several such neurons, so the parameters would add up quickly.\n",
    "        - so clearly, this **fully connectivity** is wasteful and the huge number of parameters would quickly lead to overfitting. \n",
    "\n",
    "2. ### 3D volumes of Neurons:\n",
    "    - CNN take advantage of the fact that the input consists of images and they constrain the architechture in a more sensible way.\n",
    "    - Unlike traditional neural network, CNN have neurons arranged in 3 dimensions: **width, height, depth**. For example, the example above, we would have width = 200, height = 200, and depth = 3.\n",
    "    \n",
    "3. ### Full ConvNet Architecture\n",
    "    - Convolutional Layer(CONV)\n",
    "    - Pooling Layer\n",
    "    - Fully-Connected Layer (FC)\n",
    "    - **Note**:\n",
    "        - the ConvNets transform the original image layer by layer from the original pixel values to the final class scores. Note also that some layers contain parameters and others don't. \n",
    "        - In particular, CONV/ FC layers perform transformations that are a function of *not only the activations in the input volume, but also of the parameters.*\n",
    "        - On the other hand, ReLu and POOL layers will implement a fixed function. The parameters in the CONV / FC layers will be trained with gradient descent so that the class scores that the ConvNet computes are consistent with the labels in the training set for each image.\n",
    "        - each layer may or may not have additional hyperparameters( e.g.: CONV/ FC/ POOL do, but ReLu doesn't)\n",
    "        \n",
    "4. ### Convolutional Layer: \n",
    "    - the core building block of a Convolutional Neural Network.\n",
    "    - **Local Connectivity**:\n",
    "        - we will connect each neuron to only a local region of the input volume.\n",
    "        - the spatial extent of this connectivity is a hyperparmeter called the ** receptive field ** of the neuron.\n",
    "        - the extent of the connectivity along the depth size is always equal to the depth of the input volume. \n",
    "     - <img src = \"connectivity.png\" />\n",
    "    - **Spatial Arrangement **: Three hyperparameters control the size of the output volume: the depth, steide, and zero-padding.\n",
    "        - depth: correspond to the number of filters we would like to use. Each learning to look for something different in the input. \n",
    "            - for example, if the first Convolutional Layer takes as input the raw image, then different neurons along the depth dimension may activate in presence of various oriented edges, or blobs of color. \n",
    "            - we refer to a set of neurons that are all looking at the same region of the input as a **depth column**.\n",
    "      \n",
    "        - stride: when the stride is 1 then we move the filters one pixel at a time.\n",
    "        - zero-padding: it will be convenient to pad the input volume with zeros around the border.\n",
    "            - the nice feature of zero-padding is that it will allow us to control the spatial size of the output volume. \n",
    "            - we can compute the spatial size of the output volume as a function of the input volume size (W), the receptive field size of the COnv Layer Neurons (F), the stride with which they are applied (S), and the amount of zero-padding used (P) on the border. \n",
    "            - $ spatialSize = \\frac{W - F + 2P}{S + 1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------\n",
    "Let's code the simple CNN with Tensorflow. \n",
    "This is the tutorial by [American Damien](https://github.com/aymericdamien)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TensorFlow's MNIST datasets **\n",
    "- MNIST is a classical problem that look at 28 by 28 pixel images of handwritten digits and determine which digit the image represents, for all the digits from 0-9\n",
    "- At the top of the code when you import everything, make sure you have the latest download\n",
    "   </br> `mnist = input_data.read_data_sets('/tmp/data/', one_hot=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#from __future__ import division, print_function, absolute_import\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# Import MNIST data\n",
    "mnist = input_data.read_data_sets('/tmp/data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. setting up all the hyperparameters\n",
    "learning_rate = 0.001 # alpha\n",
    "num_steps = 500 # iterations\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# 2.  Network Parameters\n",
    "num_input = 784 # 28 by 28 pixel images\n",
    "num_classes = 10 # 0 - 9 digits\n",
    "dropout = 0.75 # probability of dropping out a hidden unit\n",
    "\n",
    "# tensorflow graph input\n",
    "X = tf.placeholder(tf.float32,[None, num_input])\n",
    "Y = tf.placeholder(tf.float32,[None,num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout keep probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides =1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides = [1, strides, strides, 1], padding = 'SAME')\n",
    "    x = tf.nn.bias_add(x,b)\n",
    "    x = tf.nn.relu(x)\n",
    "    return x\n",
    "\n",
    "def maxpool2d(x,k =2):\n",
    "    # MaxPool2D Wrapper\n",
    "    return tf.nn.max_pool(x,ksize=[1,k,k,1], strides = [1,k,k,1], padding = 'SAME')\n",
    "\n",
    "# Create the model\n",
    "def conv_net (x, weihgts,biases, dropout):\n",
    "    # MNIST data input is a 1-D vectors of 784 features (28*28 pixels)\n",
    "    # Reshape to match picture formats\n",
    "    # Tensor input becomes 4D [Batch size, Height, Width, Channel]\n",
    "    x = tf.reshape(x,shape = [-1,28,28,1])\n",
    "    \n",
    "    # first layer convolution layer\n",
    "    conv1 = conv2d(x,weights['wc1'], biases['bc1'])\n",
    "     # max pooling \n",
    "    conv1 = maxpool2d(conv1, k = 2)\n",
    "   \n",
    "    conv2 = conv2d(conv1, weights['wc2'],biases['bc2'])\n",
    "    conv2 = maxpool2d(conv2,k = 2)\n",
    "    \n",
    "    # Fully connected layer\n",
    "    # reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1,weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "    \n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight and bias\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5,5,1,32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5,5,32,64])),\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024,num_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "# Construct Model\n",
    "logits = conv_net(X, weights, biases, keep_prob)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate Model\n",
    "correct_pred = tf.equal(tf.argmax(prediction,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# initialize the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Lost = 44357.6797, Training Accuracy = 0.117\n",
      "Step 10, Minibatch Lost = 24775.9453, Training Accuracy = 0.250\n",
      "Step 20, Minibatch Lost = 12615.4619, Training Accuracy = 0.414\n",
      "Step 30, Minibatch Lost = 6684.8511, Training Accuracy = 0.594\n",
      "Step 40, Minibatch Lost = 3336.0073, Training Accuracy = 0.781\n",
      "Step 50, Minibatch Lost = 4144.3545, Training Accuracy = 0.781\n",
      "Step 60, Minibatch Lost = 2608.1147, Training Accuracy = 0.805\n",
      "Step 70, Minibatch Lost = 2018.2419, Training Accuracy = 0.867\n",
      "Step 80, Minibatch Lost = 2419.0928, Training Accuracy = 0.867\n",
      "Step 90, Minibatch Lost = 2091.0078, Training Accuracy = 0.852\n",
      "Step 100, Minibatch Lost = 1925.6846, Training Accuracy = 0.898\n",
      "Step 110, Minibatch Lost = 2503.4480, Training Accuracy = 0.852\n",
      "Step 120, Minibatch Lost = 1779.9392, Training Accuracy = 0.859\n",
      "Step 130, Minibatch Lost = 2397.7441, Training Accuracy = 0.836\n",
      "Step 140, Minibatch Lost = 2417.6355, Training Accuracy = 0.875\n",
      "Step 150, Minibatch Lost = 2622.3828, Training Accuracy = 0.867\n",
      "Step 160, Minibatch Lost = 1186.7906, Training Accuracy = 0.898\n",
      "Step 170, Minibatch Lost = 1819.7654, Training Accuracy = 0.922\n",
      "Step 180, Minibatch Lost = 1846.5935, Training Accuracy = 0.898\n",
      "Step 190, Minibatch Lost = 1165.2612, Training Accuracy = 0.930\n",
      "Step 200, Minibatch Lost = 904.8927, Training Accuracy = 0.938\n",
      "Step 210, Minibatch Lost = 1034.8463, Training Accuracy = 0.914\n",
      "Step 220, Minibatch Lost = 986.6339, Training Accuracy = 0.930\n",
      "Step 230, Minibatch Lost = 676.6044, Training Accuracy = 0.914\n",
      "Step 240, Minibatch Lost = 634.8229, Training Accuracy = 0.938\n",
      "Step 250, Minibatch Lost = 191.1620, Training Accuracy = 0.977\n",
      "Step 260, Minibatch Lost = 719.2458, Training Accuracy = 0.930\n",
      "Step 270, Minibatch Lost = 1147.4185, Training Accuracy = 0.922\n",
      "Step 280, Minibatch Lost = 1937.0383, Training Accuracy = 0.883\n",
      "Step 290, Minibatch Lost = 906.7304, Training Accuracy = 0.930\n",
      "Step 300, Minibatch Lost = 490.1983, Training Accuracy = 0.953\n",
      "Step 310, Minibatch Lost = 1184.8059, Training Accuracy = 0.922\n",
      "Step 320, Minibatch Lost = 1007.7414, Training Accuracy = 0.914\n",
      "Step 330, Minibatch Lost = 686.9568, Training Accuracy = 0.930\n",
      "Step 340, Minibatch Lost = 1271.6035, Training Accuracy = 0.898\n",
      "Step 350, Minibatch Lost = 655.0744, Training Accuracy = 0.938\n",
      "Step 360, Minibatch Lost = 1481.8464, Training Accuracy = 0.922\n",
      "Step 370, Minibatch Lost = 394.9331, Training Accuracy = 0.969\n",
      "Step 380, Minibatch Lost = 429.9828, Training Accuracy = 0.945\n",
      "Step 390, Minibatch Lost = 374.5436, Training Accuracy = 0.969\n",
      "Step 400, Minibatch Lost = 754.1993, Training Accuracy = 0.945\n",
      "Step 410, Minibatch Lost = 535.3150, Training Accuracy = 0.938\n",
      "Step 420, Minibatch Lost = 790.3388, Training Accuracy = 0.922\n",
      "Step 430, Minibatch Lost = 899.8260, Training Accuracy = 0.930\n",
      "Step 440, Minibatch Lost = 668.1243, Training Accuracy = 0.945\n",
      "Step 450, Minibatch Lost = 809.9125, Training Accuracy = 0.945\n",
      "Step 460, Minibatch Lost = 621.6003, Training Accuracy = 0.938\n",
      "Step 470, Minibatch Lost = 134.5566, Training Accuracy = 0.977\n",
      "Step 480, Minibatch Lost = 667.1999, Training Accuracy = 0.945\n",
      "Step 490, Minibatch Lost = 598.1871, Training Accuracy = 0.938\n",
      "Step 500, Minibatch Lost = 669.3360, Training Accuracy = 0.961\n",
      "Optimization Finished!\n",
      "Testing Accuracy:  0.949219\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    for step in range(1,num_steps +1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        sess.run(train_op, feed_dict = {X:batch_x, Y: batch_y, keep_prob:dropout})\n",
    "        \n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch's loss value and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict = {X:batch_x, Y:batch_y, keep_prob:1.0})\n",
    "            print(\"Step \"+ str(step) + \", Minibatch Lost = \" + \n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy = \" + \"{:.3f}\".format(acc))\n",
    "            \n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    print(\"Testing Accuracy: \", sess.run(accuracy, feed_dict = {X: mnist.test.images[:256], \n",
    "                                                                Y:mnist.test.labels[:256], keep_prob:1.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
